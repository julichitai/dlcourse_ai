{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"../assignment1/data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302451, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302476, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302536, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302381, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302578, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302486, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302535, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302597, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302513, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302480, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302503, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1336f48278>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302239, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291086, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2cd0d7e60500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minitial_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate should've been reduced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.328146, Train accuracy: 0.192111, val accuracy: 0.198000\n",
      "Loss: 2.323812, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c381cf05523e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# You should see even better results than before!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# After that, implement l2 regularization on all params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/dlcourse_ai/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# the previous assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0md_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.332707, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.323381, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319965, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316467, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299728, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280305, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.262522, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.278991, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.222289, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.087136, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.237019, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.947243, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.324260, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.893347, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.330237, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.061468, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.775350, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.915968, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.115731, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.860102, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.052208, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.890259, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.273910, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.834583, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.967120, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.052431, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.936275, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.634473, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.458490, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.421627, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.517863, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.085470, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.702322, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.906487, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.862853, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.664558, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.967605, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 2.182959, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.699817, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.304242, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.896416, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.405427, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.577034, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.330723, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.086313, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 0.922169, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.722855, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.374143, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.625114, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.276963, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.669825, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.195325, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.420847, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.353648, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.138665, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.890370, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.897951, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.821726, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.740133, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.246764, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.983168, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.351728, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.010719, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.338011, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.957170, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.674561, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.182777, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.308921, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.784047, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.625534, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.925702, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.270791, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.335839, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.213184, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.560568, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.964753, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.352962, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.256456, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.865139, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.218707, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.384341, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.348448, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.584728, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.639784, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.564665, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.258021, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.628146, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.943632, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.388610, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.433870, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.185663, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.565867, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.550983, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.521585, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.805588, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.350148, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.535453, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.345001, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.533715, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.943168, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.000493, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.202010, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.465209, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.436148, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.194187, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.764112, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.411434, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.732303, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.393763, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.648262, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.161240, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.081924, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.133626, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.332158, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.486304, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.095886, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.437924, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.010066, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.834684, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.140544, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.327868, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.017265, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.459369, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.384386, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.238991, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.433861, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.413892, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.383682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.183709, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.492987, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.411896, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.185932, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.461388, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.518005, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.133352, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.299561, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.615525, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.219879, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.294895, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.208318, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.504664, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.472379, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.642254, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.459632, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.405105, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.344155, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.052136, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.104797, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.197811, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.285014, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302905, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302324, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301224, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.299621, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.297438, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294440, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.290067, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.283059, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.270657, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.246995, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.200420, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.112272, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.975686, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.845491, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.739923, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.621630, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.554910, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.585112, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.449993, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 1.435424, Train accuracy: 0.333333, val accuracy: 0.133333\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=50)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302834, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302833, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302832, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302831, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302829, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302826, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302823, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: 2.302820, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302816, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302813, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302809, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302805, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302800, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302796, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302791, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302786, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302781, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302776, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302771, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302766, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302760, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302755, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302750, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302744, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302738, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302733, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302727, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302722, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302716, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302710, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302704, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302699, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302693, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302687, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302681, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302675, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302669, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302664, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302658, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302652, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302646, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302640, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302634, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302628, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302623, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302617, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.302611, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302605, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302599, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302593, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302587, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302581, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302576, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302570, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302564, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302558, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.302552, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302546, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302541, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302535, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302529, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302523, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302517, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302512, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302506, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302500, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302494, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302488, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302483, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.302477, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302471, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302465, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302459, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302454, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302448, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302442, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302437, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302431, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302425, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302419, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302414, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302408, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302402, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302397, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.302391, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302385, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302380, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302374, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302368, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302363, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302357, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302351, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302346, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302340, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302334, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302329, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302323, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302317, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302312, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302306, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302300, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302295, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302289, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302284, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302278, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302272, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302267, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302261, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302256, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302250, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302244, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302239, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302233, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302228, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302222, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302217, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302211, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302206, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302200, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302194, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302189, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302183, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302178, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302172, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302167, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302161, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302156, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302150, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302145, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302139, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302134, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302128, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302123, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302112, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302107, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302101, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302096, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302090, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302085, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302079, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302074, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302068, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302063, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302058, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302052, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302047, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302041, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302036, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302031, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302025, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302020, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302014, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302009, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302004, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301998, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301993, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301988, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301982, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301977, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301972, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301966, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301961, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301956, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301950, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301945, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301940, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301934, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301929, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301924, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301918, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301913, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301908, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301903, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301897, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301892, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301887, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301881, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301876, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301871, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301866, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301860, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301855, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301850, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301845, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301839, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301834, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301829, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301824, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301819, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301813, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301808, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301803, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301798, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301793, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301787, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301782, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301777, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301772, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301767, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302725, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302719, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302708, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302693, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302675, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302655, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302634, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302611, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302588, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302564, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302541, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302517, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302494, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302472, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302451, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302430, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302410, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302391, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302372, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302355, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302339, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302323, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302309, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302295, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302282, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302270, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302259, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302248, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302238, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302229, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302220, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302213, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302205, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302198, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302192, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302186, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302180, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302175, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302171, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302166, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302162, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302159, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302155, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302152, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302149, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302147, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302144, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302142, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302140, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302138, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302136, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302135, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302133, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302132, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302131, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302129, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302128, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302127, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302127, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302126, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302125, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302124, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302124, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302123, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302123, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302122, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302122, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302121, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302121, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302121, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302120, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302120, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302120, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302120, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302120, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302119, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302118, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.302117, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306814, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: 2.306751, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: 2.306643, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.306506, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.306350, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.306184, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.306014, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305845, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305680, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305521, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305369, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305226, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.305091, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.304966, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.304849, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.304741, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.304642, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304550, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304466, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304389, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304319, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304255, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304197, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304144, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304095, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304052, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.304012, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303976, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303944, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303914, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303887, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303863, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.303842, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303822, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303804, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303788, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303774, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303761, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303749, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303739, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303729, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303721, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303713, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303706, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303700, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303694, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303689, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303684, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303680, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303676, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303673, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303670, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303667, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303665, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303663, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303661, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303659, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303657, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303656, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303655, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303653, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303652, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303651, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303651, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303650, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303649, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303648, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303648, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303647, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303647, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303647, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303646, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303646, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303646, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303645, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303645, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303645, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303645, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303644, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.303643, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.341905, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.339743, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.336400, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.332533, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.328531, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.324610, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.320883, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.317405, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.314191, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.311242, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.308546, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.306089, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.303854, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.301825, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.299985, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.298319, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.296811, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.295448, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.294218, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.293107, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.292105, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.291201, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.290387, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.289653, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.288992, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.288397, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.287861, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.287379, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.286944, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.286553, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.286201, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.285885, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.285600, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.285343, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.285112, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284904, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284717, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284549, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284397, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284261, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284138, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.284028, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283929, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283839, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283759, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283686, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283621, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283563, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283510, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283462, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283419, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283381, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283346, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283315, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283287, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283262, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283239, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283219, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283200, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283184, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283169, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283155, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283143, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283133, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283123, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283114, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283106, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283099, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283092, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283087, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283081, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283077, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283073, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283069, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283065, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283062, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283060, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283057, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283055, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283053, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283051, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283049, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283048, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283047, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283045, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283044, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283043, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283043, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283042, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283041, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283040, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283040, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283039, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283039, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283038, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283038, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283038, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283037, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283037, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283037, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283037, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283036, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.283035, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "best validation accuracy achieved: 0.133333\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "reg_strengths = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "learning_rate_decays = [0.999, 0.9, 0.8, 0.7, 0.5]\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "for learning_rate, reg_strength, learning_rate_decay in zip(learning_rates, reg_strengths, learning_rate_decays):\n",
    "    model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=hidden_layer_size, reg=reg_strength)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, learning_rate_decay=learning_rate_decay)\n",
    "    trainer.fit()\n",
    "    preds = model.predict(dataset.val_X)\n",
    "    accuracy = multiclass_accuracy(preds, dataset.val_y)\n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = accuracy\n",
    "        best_classifier = model\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.182000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
